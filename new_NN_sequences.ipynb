{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive sequences: 1047308\n",
      "Number of negative sequences: 6679\n"
     ]
    }
   ],
   "source": [
    "# Function to read sequences from a FASTA file\n",
    "def read_fasta(file_path):\n",
    "    try:\n",
    "        sequences = []\n",
    "        for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "            sequences.append(str(record.seq))\n",
    "        return sequences\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load positive and negative sequences\n",
    "positive_pairs = read_fasta('data/positive_pairs.fasta')\n",
    "negative_pairs = read_fasta('data/negative_pairs.fasta')\n",
    "\n",
    "print(f\"Number of positive sequences: {len(positive_pairs)}\")\n",
    "print(f\"Number of negative sequences: {len(negative_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filtered positive sequences: 1047308\n",
      "Number of filtered negative sequences: 6679\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a sequence contains only standard amino acids\n",
    "def is_valid_sequence(sequence):\n",
    "    valid_vocab = set('ABCDEFGHIKLMNPQRSTUVWYXZ_')\n",
    "    return all(char in valid_vocab for char in sequence), set(sequence) - valid_vocab\n",
    "\n",
    "# Filter the sequence dictionary\n",
    "filtered_positive = []\n",
    "filtered_negative = []\n",
    "\n",
    "for seq in positive_pairs:\n",
    "    is_valid, invalid_chars = is_valid_sequence(seq)\n",
    "    if is_valid:\n",
    "        filtered_positive.append(seq)\n",
    "    else:\n",
    "        print(f\"Sequence {seq} contains unknown amino acids: {', '.join(invalid_chars)} and will be excluded.\")\n",
    "\n",
    "for seq in negative_pairs:\n",
    "    is_valid, invalid_chars = is_valid_sequence(seq)\n",
    "    if is_valid:\n",
    "        filtered_negative.append(seq)\n",
    "    else:\n",
    "        print(f\"Sequence {seq} contains unknown amino acids: {', '.join(invalid_chars)} and will be excluded.\")\n",
    "\n",
    "print(f\"Number of filtered positive sequences: {len(filtered_positive)}\")\n",
    "print(f\"Number of filtered negative sequences: {len(filtered_negative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels\n",
    "positive_labels = [1] * len(filtered_positive)\n",
    "negative_labels = [0] * len(filtered_negative)\n",
    "\n",
    "# Combine sequences and labels\n",
    "all_sequences = filtered_positive + filtered_negative\n",
    "all_labels = positive_labels + negative_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique amino acids: {'E', 'P', 'Z', 'C', 'N', 'Q', '_', 'G', 'W', 'A', 'S', 'T', 'U', 'D', 'Y', 'I', 'K', 'L', 'V', 'X', 'M', 'B', 'H', 'F', 'R'}\n",
      "Letter to integer mapping: {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'P': 14, 'Q': 15, 'R': 16, 'S': 17, 'T': 18, 'U': 19, 'V': 20, 'W': 21, 'X': 22, 'Y': 23, 'Z': 24, '_': 25, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique amino acid letters\n",
    "all_letters = set(''.join(all_sequences))\n",
    "print(f'Unique amino acids: {all_letters}')\n",
    "\n",
    "# Create a mapping from letters to integers, reserving 0 for padding\n",
    "letter_to_int = {letter: idx + 1 for idx, letter in enumerate(sorted(all_letters))}\n",
    "letter_to_int['<PAD>'] = 0  # Padding token\n",
    "\n",
    "print(f'Letter to integer mapping: {letter_to_int}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sequences and convert to tensors (skip padding for now)\n",
    "encoded_sequences = []\n",
    "for seq in all_sequences:\n",
    "    try:\n",
    "        encoded_seq = [letter_to_int[aa] for aa in seq]\n",
    "        encoded_sequences.append(torch.tensor(encoded_seq, dtype=torch.long))\n",
    "    except KeyError as e:\n",
    "        print(f\"Error encoding sequence: {seq}. Unmapped character {e}.\")\n",
    "\n",
    "all_labels_tensor = torch.tensor(all_labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences after filtering: 1020444\n",
      "Padded sequences shape: torch.Size([1020444, 3000])\n",
      "Labels shape: torch.Size([1020444])\n"
     ]
    }
   ],
   "source": [
    "# Set maximum sequence length\n",
    "max_seq_len = 3000\n",
    "\n",
    "# Pair sequences with their corresponding labels\n",
    "filtered_sequences_with_labels = [\n",
    "    (seq, label) for seq, label in zip(encoded_sequences, all_labels_tensor) if len(seq) <= max_seq_len\n",
    "]\n",
    "\n",
    "print(f\"Number of sequences after filtering: {len(filtered_sequences_with_labels)}\")\n",
    "\n",
    "# Define a function to pad a batch of sequences and keep labels\n",
    "def batch_pad_with_labels(batch, max_length, padding_value):\n",
    "    padded_sequences = []\n",
    "    labels = []\n",
    "    for seq, label in batch:\n",
    "        padding_needed = max_length - len(seq)\n",
    "        padded_seq = torch.cat([seq, torch.full((padding_needed,), padding_value, dtype=torch.long)])\n",
    "        padded_sequences.append(padded_seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(padded_sequences), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Process sequences and labels in batches\n",
    "batch_size = 64\n",
    "padded_batches = []\n",
    "label_batches = []\n",
    "for i in range(0, len(filtered_sequences_with_labels), batch_size):\n",
    "    batch = filtered_sequences_with_labels[i:i + batch_size]\n",
    "    padded_batch, label_batch = batch_pad_with_labels(batch, max_seq_len, letter_to_int['<PAD>'])\n",
    "    padded_batches.append(padded_batch)\n",
    "    label_batches.append(label_batch)\n",
    "\n",
    "# Combine all padded batches into a single tensor\n",
    "padded_sequences = torch.cat(padded_batches, dim=0)\n",
    "labels = torch.cat(label_batches, dim=0)\n",
    "\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create a simple Dataset class\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create indices for splitting\n",
    "indices = np.arange(len(labels))\n",
    "labels_np = labels.cpu().numpy()  # Convert labels to numpy for stratification\n",
    "\n",
    "# Split indices instead of actual data\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=labels_np\n",
    ")\n",
    "\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=labels_np[temp_idx]\n",
    ")\n",
    "\n",
    "# Create datasets using indices\n",
    "train_dataset = SequenceDataset(padded_sequences[train_idx], labels[train_idx])\n",
    "val_dataset = SequenceDataset(padded_sequences[val_idx], labels[val_idx])\n",
    "test_dataset = SequenceDataset(padded_sequences[test_idx], labels[test_idx])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32  # Adjust this based on your memory constraints\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nData split summary:\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Calculate class distribution for each set\n",
    "def get_class_distribution(dataset):\n",
    "    pos = sum(label.item() == 1 for _, label in dataset)\n",
    "    total = len(dataset)\n",
    "    return pos, total - pos\n",
    "\n",
    "train_pos, train_neg = get_class_distribution(train_dataset)\n",
    "val_pos, val_neg = get_class_distribution(val_dataset)\n",
    "test_pos, test_neg = get_class_distribution(test_dataset)\n",
    "\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(f\"  Positive: {train_pos} ({train_pos/len(train_dataset)*100:.1f}%)\")\n",
    "print(f\"  Negative: {train_neg} ({train_neg/len(train_dataset)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nValidation set distribution:\")\n",
    "print(f\"  Positive: {val_pos} ({val_pos/len(val_dataset)*100:.1f}%)\")\n",
    "print(f\"  Negative: {val_neg} ({val_neg/len(val_dataset)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set distribution:\")\n",
    "print(f\"  Positive: {test_pos} ({test_pos/len(test_dataset)*100:.1f}%)\")\n",
    "print(f\"  Negative: {test_neg} ({test_neg/len(test_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"DataLoaders created:\")\n",
    "print(f\"- Training batches: {len(train_loader)}\")\n",
    "print(f\"- Validation batches: {len(val_loader)}\")\n",
    "print(f\"- Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(y_train.numpy())  # Count occurrences of each class in y_train\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = 1. / class_counts  # Inverse of the count to give more weight to minority classes\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Convert class weights to tensor\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "print(f\"Class weights tensor: {class_weights_tensor}\")\n",
    "\n",
    "# Apply weights in the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
